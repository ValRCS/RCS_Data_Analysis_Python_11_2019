{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of urls for all regions\n",
    "# for each url\n",
    "# get list of rows\n",
    "# for each row process row\n",
    "# turn each processsed list of rows into dataframe\n",
    "# concatanate dataframes together\n",
    "# perform post processing cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ss.com/lv/real-estate/farms-estates/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrlList(url, prefix='https://www.ss.com', postfix='sell/', tag='a', class_='a_category'):\n",
    "    \"\"\"\n",
    "    Returns a list of href links from a url\n",
    "    \"\"\"\n",
    "    req = requests.get(url)\n",
    "    if req.status_code != 200:\n",
    "        print(f'Unexpected status code {req.status_code}. Stopping parse')\n",
    "        return [] #return early and often principle\n",
    "    soup = BeautifulSoup(req.text, 'lxml') # could skip soup variable as well but keeping for readability\n",
    "    return [ prefix + el['href'] + postfix for el in soup.find_all(tag, class_) ]\n",
    "    # What else could we pass as argument? How could our return fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRowData(row, baseurl=\"https://www.ss.com\"):\n",
    "    return [el.text for el in row.find_all('td')[2:]] + [baseurl + row.find('a')['href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDFfromUrl(url, region = None):\n",
    "    \"\"\"\n",
    "    # get list of rows\n",
    "    # for each row process row\n",
    "    # turn each processsed list of rows into dataframe\n",
    "    \"\"\"\n",
    "    print(f'Going to gather data from URL:{url}')\n",
    "    req = requests.get(url)\n",
    "    if req.status_code != 200:\n",
    "        print(f'Unexpected status code {req.status_code}. Stopping parse')\n",
    "        return [] #return early and often principle\n",
    "    soup = BeautifulSoup(req.text, 'lxml') # could skip soup variable as well but keeping for readability\n",
    "        \n",
    "    headline = soup.find('tr', id = \"head_line\")\n",
    "    cindex = [el.text for el in headline.find_all('td')]\n",
    "    cindex[0] = cindex[0].split()[0]\n",
    "    cindex += ['URL'] #TODO add argument for this\n",
    "    cindex += ['Region']\n",
    "    \n",
    "    # TODO move it somewhere else\n",
    "    if len([el for el in soup.find_all('option') if 'Pārdod' in el.text]) == 0:\n",
    "        print(\"Oops nothing for sale\")\n",
    "        return pd.DataFrame({}, columns=cindex)\n",
    "    \n",
    "    rows = soup.find_all('tr',id = re.compile(r'tr_[0-9]+'))\n",
    "    rowsdata = [getRowData(el) for el in rows] \n",
    "    # finally we add the region if we did not have one\n",
    "    if region == None:\n",
    "        region = url.split(\"/\")[-3]\n",
    "    # we add region information at the end of our list\n",
    "    rowsdata = [el + [region] for el in rowsdata]\n",
    "    return pd.DataFrame(rowsdata, columns=cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this recipe we can append a big list of dataframes into one\n",
    "def getDFfromUrlList(urlist, delay=0.5):\n",
    "    dflist = []\n",
    "    for ur in urlist:\n",
    "        dflist.append(getDFfromUrl(ur))\n",
    "        time.sleep(delay)\n",
    "    return pd.concat(dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDF(df):\n",
    "    # for each price element, i split by empty space, get begging, get rid of comma, then cast to int\n",
    "    df[['Stāvi', 'm2']] = df[['Stāvi', 'm2']].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "    df['Price'] = df['Cena'].apply(lambda el: int(el.split(\" \")[0].replace(',','')))\n",
    "    # bonus, create currency column\n",
    "    df['Currency'] = df['Cena'].apply(lambda el: el.split(\" \")[-1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\mansprojekts\\\\mansfails.txt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"C:\\\\mansprojekts\\\\\", \"mansfails.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDF(df, path=None, name=\"Farmhouses\", my_sheet_name=\"Sheet_1\"):\n",
    "    # https://stackoverflow.com/questions/10607688/how-to-create-a-file-name-with-the-current-date-time-in-python\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # full way of writing to excel, could just do the short df.to_excel(filename)\n",
    "    fname = f'{name}_{timestr}.xlsx'\n",
    "    if path:\n",
    "        fullpath = os.path.join(path, fname)\n",
    "    else:\n",
    "        fullpath = fname\n",
    "        \n",
    "    with pd.ExcelWriter(fullpath) as writer:\n",
    "        df.to_excel(writer, sheet_name=my_sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/riga-region/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/aizkraukle-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/aluksne-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/balvi-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/bauska-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/cesis-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/daugavpils-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/dobele-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/gulbene-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/jekabpils-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/jelgava-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/kraslava-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/kuldiga-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/liepaja-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/limbadzi-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/ludza-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/madona-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/ogre-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/preili-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/rezekne-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/saldus-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/talsi-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/tukums-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/valka-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/valmiera-and-reg/sell/\n",
      "Going to gather data from URL:https://www.ss.com/lv/real-estate/farms-estates/ventspils-and-reg/sell/\n"
     ]
    }
   ],
   "source": [
    "farmurls = getUrlList(url)\n",
    "#df = getDFfromUrlList(farmurls)\n",
    "df = getDFfromUrlList(farmurls[:-1]) # we do not reallyneed the Cits region\n",
    "df = cleanDF(df)\n",
    "saveDF(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\val-p1\\\\Github\\\\RCS_Data_Analysis_Python_11_2019\\\\WebScraping'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDF(df, \"..\") # this will save one map above current map "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
